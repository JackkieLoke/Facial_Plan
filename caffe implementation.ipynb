{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "%matplotlib inline\n",
    "import caffe\n",
    "import os\n",
    "os.chdir('/home/mckc/image class/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def lenet(data_location, batch_size):\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.ImageData(batch_size=batch_size,  source=data_location,\n",
    "                             transform_param=(dict(scale=1./255,mirror=True,crop_size= 224)), ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n",
    "    n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n",
    "    n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
    "    n.relu1 = L.ReLU(n.fc1, in_place=True)\n",
    "    n.score = L.InnerProduct(n.relu1, num_output=2, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "    \n",
    "with open('lenet_auto_train.prototxt', 'w') as f:\n",
    "    f.write(str(lenet('caffe_train.txt', 2)))\n",
    "    \n",
    "with open('lenet_auto_test.prototxt', 'w') as f:\n",
    "    f.write(str(lenet('caffe_validate.txt', 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer {\r\n",
      "  name: \"data\"\r\n",
      "  type: \"ImageData\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  transform_param {\r\n",
      "    scale: 0.00392156862745\r\n",
      "    mirror: true\r\n",
      "    crop_size: 224\r\n",
      "  }\r\n",
      "  image_data_param {\r\n",
      "    source: \"caffe_train.txt\"\r\n",
      "    batch_size: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv1\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"conv1\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 20\r\n",
      "    kernel_size: 5\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool1\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"pool1\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv2\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"conv2\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 50\r\n",
      "    kernel_size: 5\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool2\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"pool2\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"fc1\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"pool2\"\r\n",
      "  top: \"fc1\"\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 500\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu1\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"fc1\"\r\n",
      "  top: \"fc1\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"score\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"fc1\"\r\n",
      "  top: \"score\"\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 2\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"loss\"\r\n",
      "  type: \"SoftmaxWithLoss\"\r\n",
      "  bottom: \"score\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"loss\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat lenet_auto_train.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "\n",
    "def solver(train_net_path, test_net_path):\n",
    "    s = caffe_pb2.SolverParameter()\n",
    "\n",
    "    # Specify locations of the train and test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "\n",
    "    s.test_interval = 10  # Test after every 1000 training iterations.\n",
    "    s.test_iter.append(250) # Test 250 \"batches\" each time we test.\n",
    "\n",
    "    s.max_iter = 10000      # # of times to update the net (training iterations)\n",
    "\n",
    "    # Set the initial learning rate for stochastic gradient descent (SGD).\n",
    "    s.base_lr = 0.0001        \n",
    "\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # Here, we 'step' the learning rate by multiplying it by a factor `gamma`\n",
    "    # every `stepsize` iterations.\n",
    "    s.lr_policy = 'step'\n",
    "    s.gamma = 0.1\n",
    "    #s.stepsize = 5000\n",
    "\n",
    "    # Set other optimization parameters. Setting a non-zero `momentum` takes a\n",
    "    # weighted average of the current gradient and previous gradients to make\n",
    "    # learning more stable. L2 weight decay regularizes learning, to help prevent\n",
    "    # the model from overfitting.\n",
    "    s.momentum = 0.9\n",
    "    s.weight_decay = 5e-4\n",
    "\n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.  Here, we'll\n",
    "    # snapshot every 10K iterations -- just once at the end of training.\n",
    "    # For larger networks that take longer to train, you may want to set\n",
    "    # snapshot < max_iter to save the network and training state to disk during\n",
    "    # optimization, preventing disaster in case of machine crashes, etc.\n",
    "    s.snapshot = 100\n",
    "    s.snapshot_prefix= \"lenet\"\n",
    "    #s.snapshot_prefix = 'examples/hdf5_classification/data/train'\n",
    "\n",
    "    # We'll train on the CPU for fair benchmarking against scikit-learn.\n",
    "    # Changing to GPU should result in much faster training!\n",
    "    #s.solver_mode = caffe_pb2.SolverParameter.CPU\n",
    "    \n",
    "    return s\n",
    "\n",
    "solver_path = 'logreg_solver.prototxt'\n",
    "with open(solver_path, 'w') as f:\n",
    "    f.write(str(solver('lenet_auto_train.prototxt', 'lenet_auto_test.prototxt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The train/test net protocol buffer definition\r\n",
      "train_net: \"lenet_auto_train.prototxt\"\r\n",
      "test_net: \"lenet_auto_test.prototxt\"\r\n",
      "# test_iter specifies how many forward passes the test should carry out.\r\n",
      "# In the case of MNIST, we have test batch size 100 and 100 test iterations,\r\n",
      "# covering the full 10,000 testing images.\r\n",
      "test_iter: 100\r\n",
      "# Carry out testing every 500 training iterations.\r\n",
      "test_interval: 50\r\n",
      "# The base learning rate, momentum and the weight decay of the network.\r\n",
      "base_lr: 0.01\r\n",
      "momentum: 0.9\r\n",
      "weight_decay: 0.0005\r\n",
      "# The learning rate policy\r\n",
      "lr_policy: \"inv\"\r\n",
      "gamma: 0.0001\r\n",
      "power: 0.75\r\n",
      "# Display every 100 iterations\r\n",
      "display: 100\r\n",
      "# The maximum number of iterations\r\n",
      "max_iter: 1000\r\n",
      "# snapshot intermediate results\r\n",
      "snapshot: 50\r\n",
      "snapshot_prefix: \"lenet\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat lenet_auto_solver.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_net: \"lenet_auto_train.prototxt\"\r\n",
      "test_net: \"lenet_auto_test.prototxt\"\r\n",
      "test_iter: 250\r\n",
      "test_interval: 10\r\n",
      "base_lr: 0.0001\r\n",
      "display: 1000\r\n",
      "max_iter: 10000\r\n",
      "lr_policy: \"step\"\r\n",
      "gamma: 0.1\r\n",
      "momentum: 0.9\r\n",
      "weight_decay: 0.0005\r\n",
      "snapshot: 100\r\n",
      "snapshot_prefix: \"lenet\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat logreg_solver.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "### load the solver and create train and test nets\n",
    "solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "#solver = caffe.SGDSolver('lenet_auto_solver.prototxt')\n",
    "solver  = caffe.SGDSolver('logreg_solver.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', (2, 3, 224, 224)),\n",
       " ('label', (2,)),\n",
       " ('conv1', (2, 20, 220, 220)),\n",
       " ('pool1', (2, 20, 110, 110)),\n",
       " ('conv2', (2, 50, 106, 106)),\n",
       " ('pool2', (2, 50, 53, 53)),\n",
       " ('fc1', (2, 500)),\n",
       " ('score', (2, 2)),\n",
       " ('loss', ())]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each output is (batch size, feature dim, spatial dim)\n",
    "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1', (20, 3, 5, 5)),\n",
       " ('conv2', (50, 20, 5, 5)),\n",
       " ('fc1', (500, 140450)),\n",
       " ('score', (2, 500))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just print the weight sizes (we'll omit the biases)\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver.net.forward()  # train net\n",
    "solver.test_nets[0].forward()  # test net (there can be more than one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': array(0.0, dtype=float32)}\n",
      "{'loss': array(43.66827392578125, dtype=float32)}\n",
      "{'loss': array(43.66827392578125, dtype=float32)}\n",
      "{'loss': array(0.0, dtype=float32)}\n",
      "{'loss': array(87.3365478515625, dtype=float32)}\n",
      "{'loss': array(87.3365478515625, dtype=float32)}\n",
      "{'loss': array(87.3365478515625, dtype=float32)}\n",
      "{'loss': array(87.3365478515625, dtype=float32)}\n",
      "{'loss': array(87.3365478515625, dtype=float32)}\n",
      "{'loss': array(87.3365478515625, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    solver.step(1)\n",
    "    print solver.net.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imshow(solver.net.params['conv1'][0].diff[:, 0].reshape(4, 5, 5, 5)\n",
    "       .transpose(0, 2, 1, 3).reshape(4*5, 5*5), cmap='gray'); axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
